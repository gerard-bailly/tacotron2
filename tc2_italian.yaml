seed: 1234
fp16_run: False
distributed_run: False
dist_backend: "nccl"
dist_url: "tcp://localhost:54321"
cudnn_enabled: True
cudnn_benchmark: Falseconfig/AD_LST/model_GST.yaml
ignore_layers: ['']

language: 'italian'

# 2 targets
dir_data: ['WAVEGLOW']
ext_data: ['WAVEGLOW']
dim_data: [80]
fe_data: [86.1328125]
use_postnet: [True]

lgs_sil_add: 0.1
nm_csv_train: 'IT.csv'
nm_csv_test: '/dev/null'
lgs_max: 20.0

# list of speakers, LC by default
nb_speakers: 3
speakers: ['LC', 'EG', 'MT']

nb_styles: 0
styles: []

################################
# Model Parameters             #
################################
symbols_embedding_dim: 512

# Prosody embedding parameters
prosody_n_convolutions: 6
prosody_conv_dim_in: [1, 32, 32, 64, 64, 128]
prosody_conv_dim_out: [32, 32, 64, 64, 128, 128]
prosody_conv_kernel: 5
prosody_conv_stride: [1, 1, 2, 2, 1, 1]

prosody_embedding_dim: 32

# Style encoder parameters
bias_encoder_with_hc: False
bias_encoder_with_entry: True # add to every output of the text encoder  
nb_style_parameters: 10
style_parameters: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
use_style_input: False
style_embedding_dim: 128

# Text encoder parameters
encoder_kernel_size: 5
encoder_n_convolutions: 3
encoder_embedding_dim: 512
encoder_lstm_hidden_dim: 256
p_encoder_dropout: 0.2

# Decoder parameters
n_frames_per_step: [2]
decoder_rnn_dim: [1024]
prenet_dim: [256]
gate_threshold: [0.5]
p_prenet_dropout: [0.25]
p_postnet_dropout: [0.25]
p_attention_dropout: [0.1]
p_decoder_dropout: [0.1]
p_teacher_forcing: [1.0]

#phonetic ouput
factor_pho: 0.00

#duration ouput
compute_durations: False
factor_dur: 1.00

# Attention parameters
attention_rnn_dim: [1024]
attention_dim: [128]

# Location Layer parameters
attention_location_n_filters: [32]
attention_location_kernel_size: [31]

# post processing network parameters
postnet_embedding_dim: [512]
postnet_kernel_size: [5]
postnet_n_convolutions: [5]

################################
# Optimization Hyperparameters #
################################
factor_gate: [1.0]
use_saved_learning_rate: True
learning_rate: !!float 1e-4
milestones: [10, 20, 50]
step_size: 100
gamma: 0.1 #learning_rate multiplied by gamma at each step_size
momentum: 0.5
weight_decay: !!float 1e-6
grad_clip_thresh: 1.0
batch_size: 16
nb_epochs: 10
mask_padding: True  # set model's padded outputs to padded values
output_alignments: False
save_embeddings: ''
